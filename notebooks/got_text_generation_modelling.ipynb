{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game ot thrones\n",
    "## Text Generation\n",
    "<hr>\n",
    "\n",
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-22 14:30:51.021842: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-22 14:31:06.078836: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-22 14:31:17.133763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../datasets/got1_sequences.txt', 'r')\n",
    "# read all text\n",
    "txtDataset = file.read()\n",
    "# close the file\n",
    "file.close()\n",
    "lstSequences = txtDataset.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Sequences\n",
    "The word embedding layer expects input sequences to be comprised of integers. We can map\n",
    "each word in our vocabulary to a unique integer and encode our input sequences. Later, when\n",
    "we make predictions, we can convert the prediction to numbers and look up their associated\n",
    "words in the same mapping. To do this encoding, we will use the Tokenizer class in the Keras\n",
    "API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lstSequences)\n",
    "sequences = tokenizer.texts_to_sequences(lstSequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary : 11941\n"
     ]
    }
   ],
   "source": [
    "print (f'Size of vocabulary : {len(tokenizer.word_index) + 1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have encoded the input sequences, we need to separate them into input (X) and\n",
    "output (y) elements, remember in the previos stage I split the text into a 50 words secuence + 1 to be the target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/hmorales/Cursos/TMLC/Game-of-thrones-Text-generation/notebooks/got_text_generation_modelling.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hmorales/Cursos/TMLC/Game-of-thrones-Text-generation/notebooks/got_text_generation_modelling.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# separate into input and output\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hmorales/Cursos/TMLC/Game-of-thrones-Text-generation/notebooks/got_text_generation_modelling.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sequences \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(sequences,dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hmorales/Cursos/TMLC/Game-of-thrones-Text-generation/notebooks/got_text_generation_modelling.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m X, y \u001b[39m=\u001b[39m sequences[:,:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], sequences[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hmorales/Cursos/TMLC/Game-of-thrones-Text-generation/notebooks/got_text_generation_modelling.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m y \u001b[39m=\u001b[39m to_categorical(y, num_classes\u001b[39m=\u001b[39m(\u001b[39mlen\u001b[39m(tokenizer\u001b[39m.\u001b[39mword_index) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hmorales/Cursos/TMLC/Game-of-thrones-Text-generation/notebooks/got_text_generation_modelling.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m seq_length \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences = np.array(sequences,dtype=object)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=(len(tokenizer.word_index) + 1))\n",
    "seq_length = X.shape[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
